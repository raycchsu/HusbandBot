{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 油價"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "import requests\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as etree\n",
    "\n",
    "\n",
    "\n",
    "class GetDataSet:\n",
    "    url = ''\n",
    "\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "\n",
    "    def get_oil_price(self):\n",
    "        try:\n",
    "            headers = {'user-agent': 'my-app/0.0.1'}\n",
    "            root = etree.fromstring(requests.get(self.url, headers=headers).text)\n",
    "            columns = [\"型別名稱\", \"產品編號\", \"產品名稱\", \"包裝\", \"銷售對象\", \"交貨地點\", \"計價單位\", \"參考牌價\", \"營業稅\", \"貨物稅\", \"牌價生效時間\", \"備註\"]\n",
    "            datatframe = pd.DataFrame(columns = columns)\n",
    "            content = ''\n",
    "\n",
    "            for node in root:\n",
    "                typeName = node.find(\"型別名稱\").text if node is not None else None\n",
    "                idNum = node.find(\"產品編號\").text if node is not None else None\n",
    "                prodName = node.find(\"產品名稱\").text if node is not None else None\n",
    "                package = node.find(\"包裝\").text if node is not None else None\n",
    "                target = node.find(\"銷售對象\").text if node is not None else None\n",
    "                local = node.find(\"交貨地點\").text if node is not None else None\n",
    "                unit = node.find(\"計價單位\").text if node is not None else None\n",
    "                ref_money = node.find(\"參考牌價\").text if node is not None else None\n",
    "                tax_1 = node.find(\"營業稅\").text if node is not None else None\n",
    "                tax_2 = node.find(\"貨物稅\").text if node is not None else None\n",
    "                time = node.find(\"牌價生效時間\").text if node is not None else None\n",
    "                note= node.find(\"備註\").text if node is not None else None\n",
    "                datatframe = datatframe.append(pd.Series([typeName, idNum, prodName, package, target, local,unit,ref_money,tax_1, tax_2, time, note], index = columns), ignore_index = True)\n",
    "                \n",
    "                #for Line output string\n",
    "                content = content + prodName + ':' + unit + ':' + ref_money +' １%0D%0A'\n",
    "            \n",
    "            return content\n",
    "            #return datatframe\n",
    "\n",
    "        except (ValueError, EOFError, KeyboardInterrupt):\n",
    "            errorMsg = '數值錯誤!請稍晚再試'\n",
    "            return errorMsg\n",
    "\n",
    "        except:\n",
    "            errorMsg = '資料擷取錯誤，請稍晚再試!'\n",
    "            return errorMsg\n",
    "            #traceback.print_exc()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98無鉛汽油:元/ 公升:20.8 １%0D%0A95無鉛汽油:元/ 公升:18.8 １%0D%0A92無鉛汽油:元/ 公升:17.3 １%0D%0A酒精汽油:元/ 公升:18.8 １%0D%0A超級柴油:元/ 公升:14.4 １%0D%0A海運輕柴油:元/ 公秉:14400 １%0D%0A海運重柴油:元/ 公秉:13900 １%0D%0A特種低硫燃料油:元/公秉:18620 １%0D%0A甲種低硫燃料油(S:0.5%):元/ 公秉:18118 １%0D%0A低硫燃料油(S:0.5%):元/ 公秉:18118 １%0D%0A發電用低硫燃料油(S:0.5%):元/ 公秉:18118 １%0D%0A低硫高流動點燃料油(S:0.5%):元/ 公秉:18068 １%0D%0A輕裂燃料油:元/ 公秉:18418 １%0D%0A\n"
     ]
    }
   ],
   "source": [
    "test = GetDataSet('https://vipmember.tmtd.cpc.com.tw/opendata/ListPriceWebService.asmx/getCPCMainProdListPrice_XML')\n",
    "content = test.get_oil_price()\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PTT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] -b BOARD_NAME\n",
      "                             (-i START_INDEX END_INDEX | -a ARTICLE_ID) [-v]\n",
      "ipykernel_launcher.py: error: the following arguments are required: -b\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mac/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2971: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "import requests\n",
    "import argparse\n",
    "import time\n",
    "import codecs\n",
    "from bs4 import BeautifulSoup\n",
    "from six import u\n",
    "\n",
    "__version__ = '1.0'\n",
    "\n",
    "# if python 2, disable verify flag in requests.get()\n",
    "VERIFY = True\n",
    "if sys.version_info[0] < 3:\n",
    "    VERIFY = False\n",
    "    requests.packages.urllib3.disable_warnings()\n",
    "\n",
    "\n",
    "class PttWebCrawler(object):\n",
    "\n",
    "    PTT_URL = 'https://www.ptt.cc'\n",
    "\n",
    "    \"\"\"docstring for PttWebCrawler\"\"\"\n",
    "    def __init__(self, cmdline=None, as_lib=False):\n",
    "        parser = argparse.ArgumentParser(formatter_class=argparse.RawDescriptionHelpFormatter, description='''\n",
    "            A crawler for the web version of PTT, the largest online community in Taiwan.\n",
    "            Input: board name and page indices (or articla ID)\n",
    "            Output: BOARD_NAME-START_INDEX-END_INDEX.json (or BOARD_NAME-ID.json)\n",
    "        ''')\n",
    "        parser.add_argument('-b', metavar='BOARD_NAME', help='Board name', required=True)\n",
    "        group = parser.add_mutually_exclusive_group(required=True)\n",
    "        group.add_argument('-i', metavar=('START_INDEX', 'END_INDEX'), type=int, nargs=2, help=\"Start and end index\")\n",
    "        group.add_argument('-a', metavar='ARTICLE_ID', help=\"Article ID\")\n",
    "        parser.add_argument('-v', '--version', action='version', version='%(prog)s ' + __version__)\n",
    "\n",
    "        if not as_lib:\n",
    "            if cmdline:\n",
    "                args = parser.parse_args(cmdline)\n",
    "            else:\n",
    "                args = parser.parse_args()\n",
    "            board = args.b\n",
    "            if args.i:\n",
    "                start = args.i[0]\n",
    "                if args.i[1] == -1:\n",
    "                    end = self.getLastPage(board)\n",
    "                else:\n",
    "                    end = args.i[1]\n",
    "                self.parse_articles(start, end, board)\n",
    "            else:  # args.a\n",
    "                article_id = args.a\n",
    "                self.parse_article(article_id, board)\n",
    "\n",
    "    def parse_articles(self, start, end, board, path='.', timeout=3):\n",
    "            filename = board + '-' + str(start) + '-' + str(end) + '.json'\n",
    "            filename = os.path.join(path, filename)\n",
    "            self.store(filename, u'{\"articles\": [', 'w')\n",
    "            for i in range(end-start+1):\n",
    "                index = start + i\n",
    "                print('Processing index:', str(index))\n",
    "                resp = requests.get(\n",
    "                    url = self.PTT_URL + '/bbs/' + board + '/index' + str(index) + '.html',\n",
    "                    cookies={'over18': '1'}, verify=VERIFY, timeout=timeout\n",
    "                )\n",
    "                if resp.status_code != 200:\n",
    "                    print('invalid url:', resp.url)\n",
    "                    continue\n",
    "                soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "                divs = soup.find_all(\"div\", \"r-ent\")\n",
    "                for div in divs:\n",
    "                    try:\n",
    "                        # ex. link would be <a href=\"/bbs/PublicServan/M.1127742013.A.240.html\">Re: [問題] 職等</a>\n",
    "                        href = div.find('a')['href']\n",
    "                        link = self.PTT_URL + href\n",
    "                        article_id = re.sub('\\.html', '', href.split('/')[-1])\n",
    "                        if div == divs[-1] and i == end-start:  # last div of last page\n",
    "                            self.store(filename, self.parse(link, article_id, board), 'a')\n",
    "                        else:\n",
    "                            self.store(filename, self.parse(link, article_id, board) + ',\\n', 'a')\n",
    "                    except:\n",
    "                        pass\n",
    "                time.sleep(0.1)\n",
    "            self.store(filename, u']}', 'a')\n",
    "            return filename\n",
    "\n",
    "    def parse_article(self, article_id, board, path='.'):\n",
    "        link = self.PTT_URL + '/bbs/' + board + '/' + article_id + '.html'\n",
    "        filename = board + '-' + article_id + '.json'\n",
    "        filename = os.path.join(path, filename)\n",
    "        self.store(filename, self.parse(link, article_id, board), 'w')\n",
    "        return filename\n",
    "\n",
    "    @staticmethod\n",
    "    def parse(link, article_id, board, timeout=3):\n",
    "        print('Processing article:', article_id)\n",
    "        resp = requests.get(url=link, cookies={'over18': '1'}, verify=VERIFY, timeout=timeout)\n",
    "        if resp.status_code != 200:\n",
    "            print('invalid url:', resp.url)\n",
    "            return json.dumps({\"error\": \"invalid url\"}, sort_keys=True, ensure_ascii=False)\n",
    "        soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "        main_content = soup.find(id=\"main-content\")\n",
    "        metas = main_content.select('div.article-metaline')\n",
    "        author = ''\n",
    "        title = ''\n",
    "        date = ''\n",
    "        if metas:\n",
    "            author = metas[0].select('span.article-meta-value')[0].string if metas[0].select('span.article-meta-value')[0] else author\n",
    "            title = metas[1].select('span.article-meta-value')[0].string if metas[1].select('span.article-meta-value')[0] else title\n",
    "            date = metas[2].select('span.article-meta-value')[0].string if metas[2].select('span.article-meta-value')[0] else date\n",
    "\n",
    "            # remove meta nodes\n",
    "            for meta in metas:\n",
    "                meta.extract()\n",
    "            for meta in main_content.select('div.article-metaline-right'):\n",
    "                meta.extract()\n",
    "\n",
    "        # remove and keep push nodes\n",
    "        pushes = main_content.find_all('div', class_='push')\n",
    "        for push in pushes:\n",
    "            push.extract()\n",
    "\n",
    "        try:\n",
    "            ip = main_content.find(text=re.compile(u'※ 發信站:'))\n",
    "            ip = re.search('[0-9]*\\.[0-9]*\\.[0-9]*\\.[0-9]*', ip).group()\n",
    "        except:\n",
    "            ip = \"None\"\n",
    "\n",
    "        # 移除 '※ 發信站:' (starts with u'\\u203b'), '◆ From:' (starts with u'\\u25c6'), 空行及多餘空白\n",
    "        # 保留英數字, 中文及中文標點, 網址, 部分特殊符號\n",
    "        filtered = [ v for v in main_content.stripped_strings if v[0] not in [u'※', u'◆'] and v[:2] not in [u'--'] ]\n",
    "        expr = re.compile(u(r'[^\\u4e00-\\u9fa5\\u3002\\uff1b\\uff0c\\uff1a\\u201c\\u201d\\uff08\\uff09\\u3001\\uff1f\\u300a\\u300b\\s\\w:/-_.?~%()]'))\n",
    "        for i in range(len(filtered)):\n",
    "            filtered[i] = re.sub(expr, '', filtered[i])\n",
    "\n",
    "        filtered = [_f for _f in filtered if _f]  # remove empty strings\n",
    "        filtered = [x for x in filtered if article_id not in x]  # remove last line containing the url of the article\n",
    "        content = ' '.join(filtered)\n",
    "        content = re.sub(r'(\\s)+', ' ', content)\n",
    "        # print 'content', content\n",
    "\n",
    "        # push messages\n",
    "        p, b, n = 0, 0, 0\n",
    "        messages = []\n",
    "        for push in pushes:\n",
    "            if not push.find('span', 'push-tag'):\n",
    "                continue\n",
    "            push_tag = push.find('span', 'push-tag').string.strip(' \\t\\n\\r')\n",
    "            push_userid = push.find('span', 'push-userid').string.strip(' \\t\\n\\r')\n",
    "            # if find is None: find().strings -> list -> ' '.join; else the current way\n",
    "            push_content = push.find('span', 'push-content').strings\n",
    "            push_content = ' '.join(push_content)[1:].strip(' \\t\\n\\r')  # remove ':'\n",
    "            push_ipdatetime = push.find('span', 'push-ipdatetime').string.strip(' \\t\\n\\r')\n",
    "            messages.append( {'push_tag': push_tag, 'push_userid': push_userid, 'push_content': push_content, 'push_ipdatetime': push_ipdatetime} )\n",
    "            if push_tag == u'推':\n",
    "                p += 1\n",
    "            elif push_tag == u'噓':\n",
    "                b += 1\n",
    "            else:\n",
    "                n += 1\n",
    "\n",
    "        # count: 推噓文相抵後的數量; all: 推文總數\n",
    "        Message_push_count = p+b+n\n",
    "        message_count = {'all': p+b+n, 'count': p-b, 'push': p, 'boo': b, \"neutral\": n}\n",
    "\n",
    "        # print 'msgs', messages\n",
    "        # print 'mscounts', message_count\n",
    "\n",
    "        # json data\n",
    "        data = {\n",
    "            'url': link,\n",
    "            'board': board,\n",
    "            'article_id': article_id,\n",
    "            'article_title': title,\n",
    "            'author': author,\n",
    "            'date': date,\n",
    "            'content': content,\n",
    "            'ip': ip,\n",
    "            'Message-push-count' : Message_push_count,\n",
    "            'message_count': message_count,\n",
    "            'messages': messages\n",
    "        }\n",
    "        # print 'original:', d\n",
    "        return json.dumps(data, sort_keys=True, ensure_ascii=False)\n",
    "   \n",
    "    @staticmethod\n",
    "    def getLastPage(board, timeout=3):\n",
    "        content = requests.get(\n",
    "            url= 'https://www.ptt.cc/bbs/' + board + '/index.html',\n",
    "            cookies={'over18': '1'}, timeout=timeout\n",
    "        ).content.decode('utf-8')\n",
    "        first_page = re.search(r'href=\"/bbs/' + board + '/index(\\d+).html\">&lsaquo;', content)\n",
    "        if first_page is None:\n",
    "            return 1\n",
    "        return int(first_page.group(1)) + 1\n",
    "\n",
    "    @staticmethod\n",
    "    def store(filename, data, mode):\n",
    "        with codecs.open(filename, mode, encoding='utf-8') as f:\n",
    "            f.write(data)\n",
    "\n",
    "    @staticmethod\n",
    "    def get(filename, mode='r'):\n",
    "        with codecs.open(filename, mode, encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    c = PttWebCrawler()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[79 push] \n",
      "https://www.ptt.cc/bbs/Gossiping/M.1588288876.A.14C.html\n",
      "\n",
      "[39 push] Re: [新聞] 勞工紓困貸款10萬元 1.9萬人申貸 首波核\n",
      "https://www.ptt.cc/bbs/Gossiping/M.1588753693.A.10F.html\n",
      "\n",
      "[32 push] [爆卦] 俄羅斯新增武漢肺炎連4天破10000人啦！\n",
      "https://www.ptt.cc/bbs/Gossiping/M.1588753416.A.0FB.html\n",
      "\n",
      "[32 push] [問卦] 100萬內買什麼車是冤大頭？\n",
      "https://www.ptt.cc/bbs/Gossiping/M.1588753501.A.B75.html\n",
      "\n",
      "[32 push] [新聞] 立委溝通公衛師法 莊競程助理爆罵邱泰源\n",
      "https://www.ptt.cc/bbs/Gossiping/M.1588753701.A.80F.html\n",
      "\n",
      "[29 push] [問卦] 35歲以上還在當業務助理的單身台女是？\n",
      "https://www.ptt.cc/bbs/Gossiping/M.1588753569.A.884.html\n",
      "\n",
      "[29 push] [問卦] 消費券858億 vs 紓困方案1兆5百億 \n",
      "https://www.ptt.cc/bbs/Gossiping/M.1588754020.A.626.html\n",
      "\n",
      "[28 push] [問卦] 太七的女生是不是很賤?\n",
      "https://www.ptt.cc/bbs/Gossiping/M.1588754010.A.F41.html\n",
      "\n",
      "[25 push] Re: [新聞] 張麗善籲：排富全面發放紓困金每人1萬元\n",
      "https://www.ptt.cc/bbs/Gossiping/M.1588753290.A.0EA.html\n",
      "\n",
      "[25 push] Re: [問卦] 北士商是一所怎麼樣的學校？\n",
      "https://www.ptt.cc/bbs/Gossiping/M.1588753438.A.1A3.html\n",
      "\n",
      "[23 push] Re: [爆卦] 中油部分站點 再度封站 問題持續無解中\n",
      "https://www.ptt.cc/bbs/Gossiping/M.1588753278.A.791.html\n",
      "\n",
      "[18 push] [問卦] 痛恨違法卻又支持違法是為什麼？\n",
      "https://www.ptt.cc/bbs/Gossiping/M.1588753361.A.F29.html\n",
      "\n",
      "[18 push] [新聞] 醫師娘爆料丈夫偷吃女病患 院方不滿名譽\n",
      "https://www.ptt.cc/bbs/Gossiping/M.1588754076.A.F1F.html\n",
      "\n",
      "[17 push] [問卦] 金融系484屌打醫學？\n",
      "https://www.ptt.cc/bbs/Gossiping/M.1588753594.A.BC1.html\n",
      "\n",
      "[16 push] Re: [新聞] 蘇貞昌：台灣經濟仍是亞洲四小龍第一！台\n",
      "https://www.ptt.cc/bbs/Gossiping/M.1588753966.A.ACF.html\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "content=''\n",
    "board = 'Gossiping'\n",
    "LastPage = PttWebCrawler.getLastPage(borad)\n",
    "PttWebCrawler(['-b', board, '-i', str(LastPage-1), str(LastPage)])\n",
    "\n",
    "with open(board + '-' + str(LastPage-2) + '-' + str(LastPage) + '.json' , 'r') as reader:\n",
    "    data = json.loads(reader.read())\n",
    "type(data)\n",
    "\n",
    "#以推文數排序\n",
    "sorted_data = sorted(data['articles'], key=lambda x : x['Message-push-count'], reverse=True)\n",
    "\n",
    "#因line無法傳送過多資訊，只取前15筆\n",
    "for article in sorted_data[0:15]:\n",
    "    data = '[{} push] {}\\n{}\\n\\n'.format(article.get('Message-push-count', None), article.get('article_title', None),\n",
    "                                             article.get('url', None))\n",
    "    content += data\n",
    "    \n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
